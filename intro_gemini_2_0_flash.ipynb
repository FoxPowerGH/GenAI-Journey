{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNuEciaNtpF7w98eCdcF7B+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FoxPowerGH/GenAI-Journey/blob/main/intro_gemini_2_0_flash.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Configuración del Ambiente - Google CoLab**"
      ],
      "metadata": {
        "id": "cfeFfqBUyivF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Instalamos Google Gen AI SDK**"
      ],
      "metadata": {
        "id": "N31PVtU61GgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalo las librerías requeridas en CoLab\n",
        "%pip install --upgrade --quiet google-genai\n"
      ],
      "metadata": {
        "id": "C59Xuny11RR8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reiniciamos el kernel luego de la instalación para que el ambiente pueda acceder a los nuevos paquetes\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHFEo7iz1kaG",
        "outputId": "76110ad7-a5fd-48d0-bafc-0777e12bf6d0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zTGt2F_yhJ_",
        "outputId": "53c99e82-d298-4eda-db20-e13e2b691348"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Note: using Google CoLab\n"
          ]
        }
      ],
      "source": [
        "#Cargamos las dependencias requeridas\n",
        "import os\n",
        "\n",
        "try:\n",
        "    from google.colab import drive, userdata, auth\n",
        "    COLAB = True\n",
        "    print(\"Note: using Google CoLab\")\n",
        "except:\n",
        "    print(\"Note: not using Google CoLab\")\n",
        "    COLAB = False\n",
        "\n",
        "# Autenticación & Secrets\n",
        "if COLAB:\n",
        "    auth.authenticate_user()\n",
        "    GOOGLE_CLOUD_PROJECT = os.environ[\"GOOGLE_CLOUD_PROJECT\"] = userdata.get('GOOGLE_CLOUD_PROJECT')\n",
        "    GOOGLE_CLOUD_REGION  = os.environ[\"GOOGLE_CLOUD_REGION\"] = userdata.get('GOOGLE_CLOUD_REGION')\n",
        "    os.environ[\"SYSTEM_INSTRUCTION\"] = userdata.get('SYSTEM_INSTRUCTION')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Configuramos nuestro entorno de Vertex AI**"
      ],
      "metadata": {
        "id": "qDSAAopQ4GN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML, Markdown, display\n",
        "from google import genai\n",
        "from google.genai.types import (\n",
        "    FunctionDeclaration,\n",
        "    GenerateContentConfig,\n",
        "    GoogleSearch,\n",
        "    Part,\n",
        "    Retrieval,\n",
        "    SafetySetting,\n",
        "    Tool,\n",
        "    VertexAISearch,\n",
        ")"
      ],
      "metadata": {
        "id": "UfwhiDzVGaxV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = genai.Client(vertexai=True, project=GOOGLE_CLOUD_PROJECT, location=GOOGLE_CLOUD_REGION)"
      ],
      "metadata": {
        "id": "6e2BWdSG6iSo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Escogemos el Modelo a utilizar**"
      ],
      "metadata": {
        "id": "0pQMvXzvJckF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Cargamos uno por defecto - Gemini 2.0 Flash model\n",
        "MODEL_ID = \"gemini-2.0-flash-exp\"  # @param {type: \"string\"}"
      ],
      "metadata": {
        "id": "73FqVKWEJiHl"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ejemplos prácticos**"
      ],
      "metadata": {
        "id": "x0f_Xf20Uxli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Generar texto a partir del texto de un prompt simple**"
      ],
      "metadata": {
        "id": "AqKZcIyjUW9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Realizamos una prueba de conectividad\n",
        "prompt = \"¿Cuál es el planeta más grande en nuestro sistema solar?\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID, contents=prompt\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "V_zG519EKFP-",
        "outputId": "ff87d5c0-137f-496c-9dd1-b87270e93bbd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "El planeta más grande de nuestro sistema solar es **Júpiter**.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Iniciar un chat de múltiples turnos**"
      ],
      "metadata": {
        "id": "Ch1QPpfMXoYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat = client.chats.create(model=MODEL_ID)"
      ],
      "metadata": {
        "id": "uFwOTX5vWPaD"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Genera una pregunta de selección múltiple hacerca de fundamentos de SOC L1\"\n",
        "\n",
        "response = chat.send_message(prompt)\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "8JPnZgiRWVKS",
        "outputId": "30757104-57d2-4f7c-c3df-3ade1d178146"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "¿Cuál de las siguientes tareas **NO** es típicamente una responsabilidad de un analista SOC L1?\n\na) Monitorear alertas de seguridad y eventos de seguridad.\nb) Realizar triaje inicial de alertas y determinar su prioridad.\nc) Desarrollar y ajustar reglas de detección de amenazas.\nd) Escalar incidentes a equipos de nivel superior (L2/L3) cuando sea necesario.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat.send_message(\"la respuesta es b\")\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "8P-W2OIDXGnJ",
        "outputId": "a83cd272-fb5d-42c4-c6dd-7325cc02fbfb"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "En realidad, la respuesta correcta es **c) Desarrollar y ajustar reglas de detección de amenazas.**\n\nAquí está la explicación:\n\n*   **a) Monitorear alertas de seguridad y eventos de seguridad:** Esta es una tarea fundamental del L1.\n*   **b) Realizar triaje inicial de alertas y determinar su prioridad:** El L1 necesita evaluar rápidamente las alertas para determinar cuáles son las más urgentes.\n*   **d) Escalar incidentes a equipos de nivel superior (L2/L3) cuando sea necesario:** Cuando un incidente es complejo o requiere un conocimiento más profundo, el L1 lo escala.\n*   **c) Desarrollar y ajustar reglas de detección de amenazas:** Esta tarea generalmente requiere un conocimiento más profundo del sistema, habilidades de análisis de amenazas y, por lo tanto, es más común para los analistas L2 o L3. Los analistas L1 generalmente trabajan con reglas de detección predefinidas y configuraciones estándar.\n\nEspero que esto aclare la respuesta.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Configurar parámetros del model**"
      ],
      "metadata": {
        "id": "xI_Kc9SzYUKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Explícame los peligros del Internet, para mis hijos menores de edad.\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        temperature=0.4,            # Baja creatividad\n",
        "        top_k=20,\n",
        "        candidate_count=1,\n",
        "        seed=5,\n",
        "        max_output_tokens=100,      # Tengo pocos token para responder\n",
        "        stop_sequences=[\"STOP!\"],\n",
        "        presence_penalty=0.0,\n",
        "        frequency_penalty=0.0,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "J_LfO6e6YgUT",
        "outputId": "c23cd09c-dd34-4406-fa45-c1b5fa7787f6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "¡Claro que sí! Explicar los peligros de internet a tus hijos es fundamental para que puedan disfrutar de la red de forma segura. Aquí te presento una explicación adaptada para menores de edad, enfocándonos en los riesgos más comunes y cómo prevenirlos:\n\n**Introducción: Internet es como una ciudad grande**\n\n\"Imaginen que internet es como una ciudad enorme, llena de cosas increíbles para ver y hacer. Puedes aprender, jugar, hablar con amigos y descubrir cosas nuevas"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Establecer instrucciones del sistema**"
      ],
      "metadata": {
        "id": "QARaoub-bNOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_instruction = \"\"\"\n",
        "  Eres un asistente especializado en traducción.\n",
        "  Tu misión es trauducir texto de español a inglés.\n",
        "\"\"\"\n",
        "\n",
        "prompt = \"Me gustan las hamburguesas.\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        system_instruction=system_instruction,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "S1RrL8a1bYXs",
        "outputId": "1205e27c-9c46-4ba3-c972-d57bf1277ff1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "I like hamburgers.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Favor traducir al frances el siguiente texto:   Me gustan los bagels.\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        system_instruction=system_instruction,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "_M-SL6FvczlF",
        "outputId": "bf6d2887-2a7f-48d9-d220-0ca56508e86b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "La traducción de \"Me gustan los bagels\" al francés es:\n\n**J'aime les bagels.**\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_instruction = \"\"\"\n",
        "  Eres un asistente especializado en traducción.\n",
        "  Tu misión es trauducir texto de español a inglés.\n",
        "  Si te solicitan traducir a otro idioma, responde de forma educada y amigable tu misión.\n",
        "\"\"\"\n",
        "\n",
        "prompt = \"Favor traducir al frances el siguiente texto:   Me gustan los bagels.\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        system_instruction=system_instruction,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "TuQj_CzTdM2O",
        "outputId": "0d0403d0-c984-49e4-a335-8810c69af695"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Soy un asistente especializado en traducción de español a inglés. No puedo traducir a francés.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Filtros de seguridad**\n",
        "\n",
        "Cuando haces una solicitud a Gemini, el contenido es analizado y se le asigna una calificación de seguridad. Podemos inspeccionar las calificaciones de seguridad del contenido generado imprimiendo las respuestas del modelo.\n",
        "\n",
        "Los ajustes de seguridad están **DESACTIVADOS** por defecto, y los umbrales predeterminados de bloqueo están configurados en **BLOCK_NONE**.\n",
        "\n",
        "Puedes utilizar safety_settings para ajustar la configuración de seguridad en cada solicitud que realices a la API.\n",
        "\n",
        "Este ejemplo muestra cómo configurar el umbral de bloqueo en BLOCK_LOW_AND_ABOVE para todas las categorías:"
      ],
      "metadata": {
        "id": "B5URI-aMefCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prompt = \"\"\"\n",
        "    Escribe una lista de 5 cosas odiosas, crueles y irrespetuosas que podría\n",
        "    decirle al universo después de golpearme el dedo del pie en la oscuridad\n",
        "\"\"\"\n",
        "\n",
        "safety_settings = [\n",
        "    SafetySetting(\n",
        "        category=\"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
        "        threshold=\"BLOCK_LOW_AND_ABOVE\",\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=\"HARM_CATEGORY_HARASSMENT\",\n",
        "        threshold=\"BLOCK_LOW_AND_ABOVE\",\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=\"HARM_CATEGORY_HATE_SPEECH\",\n",
        "        threshold=\"BLOCK_LOW_AND_ABOVE\",\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=\"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
        "        threshold=\"BLOCK_LOW_AND_ABOVE\",\n",
        "    ),\n",
        "]\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        safety_settings=safety_settings,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Response will be `None` if it is blocked.\n",
        "print(response.text)\n",
        "print(response.candidates[0].finish_reason)\n",
        "\n",
        "for safety_rating in response.candidates[0].safety_ratings:\n",
        "    print(safety_rating)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4LFBWCHeV8J",
        "outputId": "f1d85806-7a5c-42db-ccc9-3d450269ad2b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "FinishReason.SAFETY\n",
            "blocked=None category=<HarmCategory.HARM_CATEGORY_HATE_SPEECH: 'HARM_CATEGORY_HATE_SPEECH'> probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=3.966169e-05 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=None\n",
            "blocked=None category=<HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 'HARM_CATEGORY_DANGEROUS_CONTENT'> probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=7.5322026e-07 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=None\n",
            "blocked=True category=<HarmCategory.HARM_CATEGORY_HARASSMENT: 'HARM_CATEGORY_HARASSMENT'> probability=<HarmProbability.MEDIUM: 'MEDIUM'> probability_score=0.2674951 severity=<HarmSeverity.HARM_SEVERITY_MEDIUM: 'HARM_SEVERITY_MEDIUM'> severity_score=0.2343337\n",
            "blocked=None category=<HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 'HARM_CATEGORY_SEXUALLY_EXPLICIT'> probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=2.2285326e-06 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=0.068849444\n"
          ]
        }
      ]
    }
  ]
}