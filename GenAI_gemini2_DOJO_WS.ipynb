{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9e3I5AkJ8XJJ33Cui57Ed",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FoxPowerGH/GenAI-Journey/blob/main/GenAI_gemini2_DOJO_WS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Configuración del Ambiente - Google CoLab**"
      ],
      "metadata": {
        "id": "cfeFfqBUyivF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Instalamos Google Gen AI SDK**\n",
        "\n",
        "La documentación:  [Google AI for Developers](https://ai.google.dev/gemini-api/docs?hl=es-419)"
      ],
      "metadata": {
        "id": "N31PVtU61GgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalo las librerías requeridas en CoLab\n",
        "%pip install -U -q 'google-genai'\n"
      ],
      "metadata": {
        "id": "C59Xuny11RR8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6144b54-dd4f-49d8-93b2-40ccc703a5c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/130.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.7/130.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reiniciamos el kernel luego de la instalación para que el ambiente pueda acceder a los nuevos paquetes\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHFEo7iz1kaG",
        "outputId": "2a335ce5-297a-4bfe-efe5-4d941bd0b386"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Configuramos nuestra API Key**"
      ],
      "metadata": {
        "id": "qDSAAopQ4GN0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zTGt2F_yhJ_",
        "outputId": "07bed953-d079-48b9-b295-2b4f197e4b3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Note: using Google CoLab\n"
          ]
        }
      ],
      "source": [
        "#Cargamos las dependencias requeridas\n",
        "import os\n",
        "\n",
        "try:\n",
        "    COLAB = True\n",
        "    print(\"Note: using Google CoLab\")\n",
        "except:\n",
        "    print(\"Note: not using Google CoLab\")\n",
        "    COLAB = False\n",
        "\n",
        "# API KEY & Secrets\n",
        "if COLAB:\n",
        "    from google.colab import userdata\n",
        "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Iniciamos el cliente del SDK**"
      ],
      "metadata": {
        "id": "5xX5EHNQoAAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)\n"
      ],
      "metadata": {
        "id": "UfwhiDzVGaxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Escogemos el Modelo a utilizar**"
      ],
      "metadata": {
        "id": "0pQMvXzvJckF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Cargamos uno por defecto - Gemini 2.0 Flash model\n",
        "MODEL_ID = \"gemini-2.0-flash-exp\"  # @param {type: \"string\"}"
      ],
      "metadata": {
        "id": "73FqVKWEJiHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Comprobamos que podemos utilizar el LLM a partir del texto de un prompt simple**"
      ],
      "metadata": {
        "id": "AqKZcIyjUW9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Realizamos una prueba de conectividad\n",
        "from IPython.display import Markdown\n",
        "prompt = \"¿Cuál es el planeta más grande en nuestro sistema solar?\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID, contents=prompt\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "V_zG519EKFP-",
        "outputId": "dd1abcfb-f5cd-4c27-e2e8-5a08970d79d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "El planeta más grande de nuestro sistema solar es **Júpiter**.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ejemplos prácticos**"
      ],
      "metadata": {
        "id": "x0f_Xf20Uxli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Iniciar un chat de múltiples turnos**"
      ],
      "metadata": {
        "id": "Ch1QPpfMXoYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat = client.chats.create(model=MODEL_ID)"
      ],
      "metadata": {
        "id": "uFwOTX5vWPaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Genera una pregunta de selección múltiple acerca de fundamentos de SOC L1\"\n",
        "\n",
        "response = chat.send_message(prompt)\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "8JPnZgiRWVKS",
        "outputId": "f5961f1b-d749-4cf5-b4d9-ffd85adb55f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "¿Cuál de las siguientes tareas es **menos probable** que realice un Analista SOC L1 en su rol diario?\n\na) Monitorear alertas de seguridad y correlacionarlas.\nb) Realizar análisis forense detallado de malware complejo.\nc) Escalar incidentes a niveles superiores para su investigación.\nd) Seguir playbooks y procedimientos operativos estándar (SOPs) predefinidos.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat.send_message(\"la respuesta es b\")\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "8P-W2OIDXGnJ",
        "outputId": "2cf5d079-c659-475c-e145-2fd9ecb3e294",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Correcto. La respuesta es **b) Realizar análisis forense detallado de malware complejo.**\n\nAquí está la justificación:\n\n*   **Analistas SOC L1 (Nivel 1)** generalmente son el primer punto de contacto en el SOC. Su enfoque principal está en la detección inicial, el triaje básico y la escalación. No suelen tener la experiencia ni la autorización para realizar análisis forenses profundos.\n\n*   **a) Monitorear alertas de seguridad y correlacionarlas:** Esta es una tarea fundamental para identificar posibles incidentes.\n\n*   **c) Escalar incidentes a niveles superiores para su investigación:**  Cuando un analista L1 identifica un incidente que requiere un análisis más profundo, lo escala a los analistas L2 o L3.\n\n*   **d) Seguir playbooks y procedimientos operativos estándar (SOPs) predefinidos:** Los SOPs son cruciales para asegurar la consistencia y la eficiencia en las operaciones del SOC.\n\nPor lo tanto, el análisis forense detallado de malware complejo es más propio de un Analista SOC L2 o L3, o incluso de un especialista en respuesta a incidentes.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Configurar parámetros del model**"
      ],
      "metadata": {
        "id": "xI_Kc9SzYUKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Explícame los peligros del Internet, para mis hijos menores de edad.\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=types.GenerateContentConfig(\n",
        "        temperature=0.4,            # Baja creatividad\n",
        "        top_k=20,\n",
        "        top_p= 0.5,\n",
        "        candidate_count=1,\n",
        "        seed=5,\n",
        "        max_output_tokens=100,      # Tengo pocos token para responder\n",
        "        stop_sequences=[\"STOP!\"],\n",
        "        presence_penalty=0.0,\n",
        "        frequency_penalty=0.0,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "J_LfO6e6YgUT",
        "outputId": "6232cb05-91aa-41cf-90f2-b6945cfc17a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "¡Hola! Es muy importante hablar con tus hijos sobre los peligros de Internet, adaptando el lenguaje a su edad. Aquí te presento una explicación que puedes adaptar, dividida por temas y con ejemplos:\n\n**1. Contacto con desconocidos (Peligro de grooming y ciberacoso):**\n\n*   **Explicación:** \"Imagina que en la calle no hablarías con cualquier persona, ¿verdad? Pues en Internet es igual. Hay personas que no son quienes"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Establecer instrucciones del sistema**"
      ],
      "metadata": {
        "id": "QARaoub-bNOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_instruction = \"\"\"\n",
        "  Eres un asistente especializado en traducción.\n",
        "  Tu misión es trauducir texto de español a inglés.\n",
        "\"\"\"\n",
        "\n",
        "prompt = \"Me gustan las hamburguesas.\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=types.GenerateContentConfig(\n",
        "        system_instruction=system_instruction,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "S1RrL8a1bYXs",
        "outputId": "f8bd8b28-7863-4acc-95be-c5cadfef588d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "I like hamburgers.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Favor traducir al frances el siguiente texto:   Me gustan los bagels.\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=types.GenerateContentConfig(\n",
        "        system_instruction=system_instruction,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "_M-SL6FvczlF",
        "outputId": "05b1ee01-b4cc-4bab-e17f-6a16d2ed2070"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Por supuesto, aquí está la traducción de \"Me gustan los bagels\" al francés:\n\n\"J'aime les bagels.\"\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_instruction = \"\"\"\n",
        "  Eres un asistente especializado en traducción.\n",
        "  Tu misión es trauducir texto de español a inglés.\n",
        "  Si te solicitan traducir a otro idioma, responde de forma educada y amigable tu misión.\n",
        "\"\"\"\n",
        "\n",
        "prompt = \"Favor traducir al frances el siguiente texto:   Me gustan los bagels.\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=types.GenerateContentConfig(\n",
        "        system_instruction=system_instruction,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "TuQj_CzTdM2O",
        "outputId": "0d0403d0-c984-49e4-a335-8810c69af695"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Soy un asistente especializado en traducción de español a inglés. No puedo traducir a francés.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Filtros de seguridad**\n",
        "\n",
        "Cuando haces una solicitud a Gemini, el contenido es analizado y se le asigna una calificación de seguridad. Podemos inspeccionar las calificaciones de seguridad del contenido generado imprimiendo las respuestas del modelo.\n",
        "\n",
        "Los ajustes de seguridad están **DESACTIVADOS** por defecto, y los umbrales predeterminados de bloqueo están configurados en **BLOCK_NONE**.\n",
        "\n",
        "Puedes utilizar safety_settings para ajustar la configuración de seguridad en cada solicitud que realices a la API.\n",
        "\n",
        "Este ejemplo muestra cómo configurar el umbral de bloqueo en BLOCK_LOW_AND_ABOVE para todas las categorías:"
      ],
      "metadata": {
        "id": "B5URI-aMefCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prompt = \"\"\"\n",
        "    Escribe una lista de 5 cosas odiosas, crueles y irrespetuosas que podría\n",
        "    decirle al universo después de golpearme el dedo del pie en la oscuridad\n",
        "\"\"\"\n",
        "\n",
        "safety_settings = [\n",
        "    SafetySetting(\n",
        "        category=\"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
        "        threshold=\"BLOCK_LOW_AND_ABOVE\",\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=\"HARM_CATEGORY_HARASSMENT\",\n",
        "        threshold=\"BLOCK_LOW_AND_ABOVE\",\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=\"HARM_CATEGORY_HATE_SPEECH\",\n",
        "        threshold=\"BLOCK_LOW_AND_ABOVE\",\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=\"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
        "        threshold=\"BLOCK_LOW_AND_ABOVE\",\n",
        "    ),\n",
        "]\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        safety_settings=safety_settings,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Response will be `None` if it is blocked.\n",
        "print(response.text)\n",
        "print(response.candidates[0].finish_reason)\n",
        "\n",
        "for safety_rating in response.candidates[0].safety_ratings:\n",
        "    print(safety_rating)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4LFBWCHeV8J",
        "outputId": "f1d85806-7a5c-42db-ccc9-3d450269ad2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "FinishReason.SAFETY\n",
            "blocked=None category=<HarmCategory.HARM_CATEGORY_HATE_SPEECH: 'HARM_CATEGORY_HATE_SPEECH'> probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=3.966169e-05 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=None\n",
            "blocked=None category=<HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 'HARM_CATEGORY_DANGEROUS_CONTENT'> probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=7.5322026e-07 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=None\n",
            "blocked=True category=<HarmCategory.HARM_CATEGORY_HARASSMENT: 'HARM_CATEGORY_HARASSMENT'> probability=<HarmProbability.MEDIUM: 'MEDIUM'> probability_score=0.2674951 severity=<HarmSeverity.HARM_SEVERITY_MEDIUM: 'HARM_SEVERITY_MEDIUM'> severity_score=0.2343337\n",
            "blocked=None category=<HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 'HARM_CATEGORY_SEXUALLY_EXPLICIT'> probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=2.2285326e-06 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=0.068849444\n"
          ]
        }
      ]
    }
  ]
}